{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26b6366",
   "metadata": {},
   "source": [
    "# 4. Test the Full Pipeline: Detection and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793901f",
   "metadata": {},
   "source": [
    "This notebook runs the complete vehicle processing pipeline:\n",
    "1. **Detection**: It uses the fine-tuned YOLO model to detect vehicles in the `traffic_test.mp4` video.\n",
    "2. **Classification**: For each detected vehicle, it uses the fine-tuned `EfficientNetB4_CBAM` classifier to identify the car's make and model.\n",
    "\n",
    "The final output is a video with bounding boxes and class labels drawn on each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from IPython.display import Video, display\n",
    "import os\n",
    "\n",
    "# Import the model definition from your project files\n",
    "from models.classification.model import EfficientNetB4_CBAM\n",
    "\n",
    "# --- Configuration ---\n",
    "DETECTION_MODEL_PATH = \"models/detection/yolo_finetune/vehicle_detection/weights/best.pt\"\n",
    "CLASSIFICATION_MODEL_PATH = \"models/classification/results/carvit_model_20250624_014531_best_acc.pth\"\n",
    "VIDEO_PATH = \"test_traffic_speedup.mp4\"\n",
    "OUTPUT_VIDEO_PATH = \"traffic_test_classified.mp4\"\n",
    "\n",
    "# This should be the same list of classes used to train the classifier\n",
    "# You might need to load this from a file or define it as it was in your training script\n",
    "CLASS_NAME_PATH = \"models/classification/class_names.txt\"\n",
    "with open(CLASS_NAME_PATH, \"r\") as f:\n",
    "    CLASS_NAMES = [line.strip() for line in f.readlines()]\n",
    "NUM_CLASSES = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c2da",
   "metadata": {},
   "source": [
    "### Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO Detection Model\n",
    "print(f\"Loading detection model from: {DETECTION_MODEL_PATH}\")\n",
    "detection_model = YOLO(DETECTION_MODEL_PATH)\n",
    "\n",
    "# Load EfficientNetB4_CBAM Classification Model\n",
    "print(f\"Loading classification model from: {CLASSIFICATION_MODEL_PATH}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classification_model = EfficientNetB4_CBAM(num_classes=NUM_CLASSES)\n",
    "if torch.cuda.is_available():\n",
    "    classification_model.load_state_dict(torch.load(CLASSIFICATION_MODEL_PATH))\n",
    "else:\n",
    "    classification_model.load_state_dict(torch.load(CLASSIFICATION_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "classification_model.eval() # Set model to evaluation mode\n",
    "classification_model.to(device)\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea39dd3",
   "metadata": {},
   "source": [
    "### Define Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab51fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the same transformations used during the classification model training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ecfbc",
   "metadata": {},
   "source": [
    "### Process the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Processing video: {VIDEO_PATH}...\")\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 1. Run Detection\n",
    "        detection_results = detection_model(frame, verbose=False)\n",
    "\n",
    "        # 2. Process each detection\n",
    "        for result in detection_results:\n",
    "            for box in result.boxes:\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "                # Crop the detected vehicle\n",
    "                vehicle_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # 3. Classify the vehicle\n",
    "                pil_img = Image.fromarray(cv2.cvtColor(vehicle_crop, cv2.COLOR_BGR2RGB))\n",
    "                input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = classification_model(input_tensor)\n",
    "                    _, predicted_idx = torch.max(outputs, 1)\n",
    "                    class_name = CLASS_NAMES[predicted_idx.item()]\n",
    "\n",
    "                # 4. Draw bounding box and class label on the frame\n",
    "                label = f'Car: {class_name}'\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Finished processing. Classified video saved to: {OUTPUT_VIDEO_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a99c1",
   "metadata": {},
   "source": [
    "### Display the Final Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OUTPUT_VIDEO_PATH):\n",
    "    display(Video(OUTPUT_VIDEO_PATH, embed=True, width=640))\n",
    "else:\n",
    "    print(f\"Output video not found at: {OUTPUT_VIDEO_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
