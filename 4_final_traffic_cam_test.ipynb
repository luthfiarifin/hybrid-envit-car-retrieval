{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26b6366",
   "metadata": {},
   "source": [
    "# Test the Full Pipeline: Detection and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793901f",
   "metadata": {},
   "source": [
    "This notebook runs the complete vehicle processing pipeline:\n",
    "1. **Detection**: It uses the fine-tuned YOLO model to detect vehicles in the `traffic_test.mp4` video.\n",
    "2. **Classification**: For each detected vehicle, it uses the fine-tuned `EfficientNetB4_CBAM` classifier to identify the car's make and model.\n",
    "\n",
    "The final output is a video with bounding boxes and class labels drawn on each frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938da3b2-cb0a-4722-9df9-9388a645a7fa",
   "metadata": {},
   "source": [
    "### Clone Model Code Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8a5d41-6819-4991-aa3b-0475ef69a1f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T12:02:23.081766Z",
     "iopub.status.busy": "2025-06-24T12:02:23.081467Z",
     "iopub.status.idle": "2025-06-24T12:02:37.587081Z",
     "shell.execute_reply": "2025-06-24T12:02:37.586379Z",
     "shell.execute_reply.started": "2025-06-24T12:02:23.081733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'code'...\n",
      "remote: Enumerating objects: 266, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 266 (delta 0), reused 0 (delta 0), pack-reused 265 (from 2)\u001b[K\n",
      "Receiving objects: 100% (266/266), 500.85 MiB | 47.01 MiB/s, done.\n",
      "Resolving deltas: 100% (99/99), done.\n",
      "Updating files: 100% (70/70), done.\n",
      "1_run_scraper_into_dataset.ipynb\tcode\t\t requirements.txt\n",
      "2_finetune_the_detection_model.ipynb\tdata_processing  traffic_test.mp4\n",
      "3_train_the_classification_model.ipynb\tlogs\n",
      "4_final_traffic_cam_test.ipynb\t\tmodels\n"
     ]
    }
   ],
   "source": [
    "!git clone -b feat/pretrained-model https://github.com/luthfiarifin/hybrid-envit-car-retrieval.git code\n",
    "!mv code/* .\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d01b830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T12:02:40.800561Z",
     "iopub.status.busy": "2025-06-24T12:02:40.800274Z",
     "iopub.status.idle": "2025-06-24T12:02:44.868154Z",
     "shell.execute_reply": "2025-06-24T12:02:44.867374Z",
     "shell.execute_reply.started": "2025-06-24T12:02:40.800536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from IPython.display import Video, display\n",
    "import os\n",
    "\n",
    "# Import the model definition from your project files\n",
    "from models.classification.model import EfficientNetB4_CBAM\n",
    "\n",
    "# --- Configuration ---\n",
    "DETECTION_MODEL_PATH = \"models/detection/yolo_finetune/vehicle_detection/weights/best.pt\"\n",
    "CLASSIFICATION_MODEL_PATH = \"models/classification/results/car_classifier_model_20250624_014531_best_acc.pth\"\n",
    "VIDEO_PATH = \"traffic_test.mp4\"\n",
    "OUTPUT_VIDEO_PATH = \"traffic_test_classified.mp4\"\n",
    "\n",
    "# This should be the same list of classes used to train the classifier\n",
    "# You might need to load this from a file or define it as it was in your training script\n",
    "CLASS_NAME_PATH = \"models/classification/class_names.txt\"\n",
    "with open(CLASS_NAME_PATH, \"r\") as f:\n",
    "    CLASS_NAMES = [line.strip() for line in f.readlines()]\n",
    "NUM_CLASSES = len(CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c2da",
   "metadata": {},
   "source": [
    "### Load the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27b94e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T12:02:44.869539Z",
     "iopub.status.busy": "2025-06-24T12:02:44.869154Z",
     "iopub.status.idle": "2025-06-24T12:02:46.058911Z",
     "shell.execute_reply": "2025-06-24T12:02:46.058246Z",
     "shell.execute_reply.started": "2025-06-24T12:02:44.869520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detection model from: models/detection/yolo_finetune/vehicle_detection/weights/best.pt\n",
      "Loading classification model from: models/classification/results/car_classifier_model_20250624_014531_best_acc.pth\n",
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load YOLO Detection Model\n",
    "print(f\"Loading detection model from: {DETECTION_MODEL_PATH}\")\n",
    "detection_model = YOLO(DETECTION_MODEL_PATH)\n",
    "\n",
    "# Load EfficientNetB4_CBAM Classification Model\n",
    "print(f\"Loading classification model from: {CLASSIFICATION_MODEL_PATH}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classification_model = EfficientNetB4_CBAM(num_classes=NUM_CLASSES)\n",
    "if torch.cuda.is_available():\n",
    "    classification_model.load_state_dict(torch.load(CLASSIFICATION_MODEL_PATH))\n",
    "else:\n",
    "    classification_model.load_state_dict(torch.load(CLASSIFICATION_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "classification_model.eval() # Set model to evaluation mode\n",
    "classification_model.to(device)\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea39dd3",
   "metadata": {},
   "source": [
    "### Define Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab51fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T12:02:46.059977Z",
     "iopub.status.busy": "2025-06-24T12:02:46.059764Z",
     "iopub.status.idle": "2025-06-24T12:02:46.064122Z",
     "shell.execute_reply": "2025-06-24T12:02:46.063372Z",
     "shell.execute_reply.started": "2025-06-24T12:02:46.059960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the same transformations used during the classification model training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ecfbc",
   "metadata": {},
   "source": [
    "### Process the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552c7bdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T12:02:46.065167Z",
     "iopub.status.busy": "2025-06-24T12:02:46.064975Z",
     "iopub.status.idle": "2025-06-24T12:20:50.650644Z",
     "shell.execute_reply": "2025-06-24T12:20:50.649757Z",
     "shell.execute_reply.started": "2025-06-24T12:02:46.065152Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: traffic_test.mp4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 5920/5920 [18:04<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing. Classified video saved to: traffic_test_classified.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F  # For softmax\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Processing video: {VIDEO_PATH}...\")\n",
    "\n",
    "with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 1. Run Detection\n",
    "        detection_results = detection_model(frame, verbose=False)\n",
    "\n",
    "        # 2. Process each detection\n",
    "        for result in detection_results:\n",
    "            for box in result.boxes:\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "                # Crop the detected vehicle\n",
    "                vehicle_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # 3. Classify the vehicle\n",
    "                pil_img = Image.fromarray(cv2.cvtColor(vehicle_crop, cv2.COLOR_BGR2RGB))\n",
    "                input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = classification_model(input_tensor)\n",
    "                    probabilities = F.softmax(outputs[0], dim=0)\n",
    "                    top1_prob, predicted_idx = torch.topk(probabilities, 1)\n",
    "                    class_name = CLASS_NAMES[predicted_idx.item()]\n",
    "                    confidence_score = top1_prob.item()\n",
    "\n",
    "                # 4. Draw bounding box and class label on the frame\n",
    "                label = f'Car: {class_name} ({confidence_score:.2f})'\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(frame)\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Finished processing. Classified video saved to: {OUTPUT_VIDEO_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
